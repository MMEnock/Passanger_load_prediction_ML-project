{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877bd9b6",
   "metadata": {},
   "source": [
    "## <span style='color:green'> Goals of the project </span>\n",
    "This is a Data Cleaning and merging operation aimed at putting all Flight Data sets together and finally cutting them into **Depature** and **Arrival** flight schedules. \n",
    "\n",
    "It has different Data Bases which will be cleaned and then further merged . At the end of , the files will be stored and further Exploration Data Anaylsis will be deployed . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa583eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np # importing the necessary libararies for the given project \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns \n",
    "import dateutil.parser as parser \n",
    "import glob\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime as dt, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f2ad077",
   "metadata": {},
   "source": [
    "# Directory containing the CSV files\n",
    "Dir = r\"D:\\DEHLI Machine Learning Projects\\AOBD_Historical_Data\\NF\"\n",
    "csv_file_list = glob.glob(Dir + '/*.csv')\n",
    "\n",
    "# List to hold dataframes for each CSV file\n",
    "dataframes = []\n",
    "\n",
    "# Read each CSV file and append to the dataframes list\n",
    "for file in csv_file_list:\n",
    "    df = pd.read_csv(file , low_memory=False)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Convert 'gate_arrival' column to numeric, setting errors='coerce' to handle non-numeric values\n",
    "if 'gate_arrival' in merged_df.columns:\n",
    "    merged_df = merged_df.drop(columns= ['gate_arrival','runway','gates_departure'])\n",
    "     \n",
    "\n",
    "# Save the concatenated dataframe as a Parquet file\n",
    "parquet_file_path = Dir + '/merged.parquet'\n",
    "merged_df.to_parquet(parquet_file_path)\n",
    "\n",
    "print(f'Merged Parquet file saved as: {parquet_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97918361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the start time in order to understand how long the model performs. \n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3decb05f",
   "metadata": {},
   "source": [
    "## 1.0 First section of Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3430cef",
   "metadata": {},
   "source": [
    "### 1.1 Data Importation from saved Directory \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65babd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is saved in a parquet format to \n",
    "NF = pd.read_parquet(\n",
    "    r'D:\\DEHLI Machine Learning Projects\\AOBD_Historical_Data\\NF/merged.parquet'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae9e0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Data frame is (729971, 28)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the Data frame is {NF.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b347e",
   "metadata": {},
   "source": [
    "### 1.2 The below functions do the following : \n",
    "- Renaming and  and deleting out incorrect Terminals\n",
    "- Converting Dates into the correct Pandas format \n",
    "- Creating new columns to represent the Indian local times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c210725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_terminals_name(df,column_name): \n",
    "    '''\n",
    "    The goal of this function is to change number terminals to correct terminal names . The data was written in \n",
    "    a wring format and for that matter, It has to be changed from nummbers to T-number formats. \n",
    "    '''\n",
    "    if column_name in df.columns:\n",
    "    # Replace '3' with 'T3' and '2' with 'T2'\n",
    "        df[column_name] = df[column_name].replace({'3': 'T3', '2': 'T2','C':'T1','D':'T1'})\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7112be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the correct_terminals_name\n",
    "NF = correct_terminals_name(NF,\"public_terminal\") \n",
    "# The plan is to create a delete_terminals function to remove the F, x and  non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cb31e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_incorrect_terminals(df, column_name): \n",
    "    '''\n",
    "    The delete_incorrect_terminals function is aimed at removing values such as F, X, G, \n",
    "    as well as None, that do not represent airport terminals.\n",
    "    '''\n",
    "    # Convert the column to string and filter out rows where the column's value ends with specified characters\n",
    "    mask = df[column_name].astype(str).str.endswith((' ', 'X', 'F', 'G', 'J', 'P', 'H'))\n",
    "    df.drop(df[mask].index, inplace=True)\n",
    "    \n",
    "    # Drop rows with NaN values in the specified column\n",
    "    df.dropna(subset=[column_name], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a33af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NF = delete_incorrect_terminals(NF,\"public_terminal\") # Applying the function to remove unecessary terminals . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fccff797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_pandas_format(df, column_name):\n",
    "    '''\n",
    "    The goal of this function is to convert the date of flight depature and arrival to a pandas understood format\n",
    "    '''\n",
    "    result = pd.to_datetime(df[column_name],\n",
    "                           # dayfirst=True,\n",
    "                            format='mixed',\n",
    "                            errors=\"coerce\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a650f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "NF = convert_date_pandas_format(NF, 'sibt')\n",
    "NF = convert_date_pandas_format(NF, 'sobt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59c1fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def timeChange(dt):\n",
    "    '''\n",
    "    The goal of this function is to convert the uct time to the ugandan local time. The function returns\n",
    "    an updated time with an addition time of 5 hours and 30 minutes . \n",
    "    '''\n",
    "    if pd.isna(dt):\n",
    "        return \"NAT\"\n",
    "    else:\n",
    "        dt = pd.to_datetime(dt,dayfirst=True)  # Convert dt to datetime object\n",
    "        updated_time = dt + datetime.timedelta(\n",
    "            hours=5, minutes=30)  #time_change had been taken of by dt..\n",
    "        return updated_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee841525",
   "metadata": {},
   "outputs": [],
   "source": [
    "NF[\"sibt_uct\"] = NF[\"sibt\"].apply(lambda x: timeChange(x))\n",
    "NF[\"sobt_uct\"] = NF[\"sobt\"].apply(lambda x: timeChange(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541a6fe",
   "metadata": {},
   "source": [
    "### 1.3 The below functions take care of the following tasks \n",
    "- Select specific columns that will be used for further Data Manipulation .\n",
    "- Sub-devide the Data Frame into Arrival and Deapture sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3adea806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_specific_legtype(df, leg_type, column_name, C_1, C_2, C_3, C_4, C_5,\n",
    "                            C_6, C_7, C_8, C_9, C_10):\n",
    "    '''\n",
    "    The goal of this function is to sub-devide the df into two parts , Arrival and Deaptures filled with column_name.\n",
    "    the df represents the Data Frame and leg_type is used to cut the df into those two parts . \n",
    "    '''\n",
    "    filtered_df = df[df[leg_type] == column_name]\n",
    "    filtered_df[C_1] = filtered_df[C_1].apply(pd.to_datetime)\n",
    "    filtered_df.loc[:, 'Date'] = filtered_df.loc[:, C_1].dt.date.apply(\n",
    "        pd.to_datetime)\n",
    "    filtered_df = filtered_df[[\n",
    "        C_1, 'Date', C_2, C_3, C_4, C_5, C_6, C_7, C_8, C_9, C_10\n",
    "    ]]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "836832bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NF_Arrival = choose_specific_legtype(NF, 'leg_type', \"ARRIVAL\", 'sibt_uct',\n",
    "                                     'all_capacity_pax', 'flight_number',\n",
    "                                     'origin_airport_iata',\n",
    "                                     'destination_airport_iata', 'leg_type',\n",
    "                                     \"traffic_type\", \"all_boarded_pax\",\n",
    "                                     \"public_terminal\", \"airline_iata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af43f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NF_Depature = choose_specific_legtype(NF, 'leg_type', \"DEPARTURE\", 'sobt_uct',\n",
    "                                     'all_capacity_pax', 'flight_number',\n",
    "                                     'origin_airport_iata',\n",
    "                                     'destination_airport_iata', 'leg_type',\n",
    "                                     \"traffic_type\", \"all_boarded_pax\",\n",
    "                                     \"public_terminal\", \"airline_iata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00978982",
   "metadata": {},
   "source": [
    "### 1.4 Importing calender information in order to merge it up with the date\n",
    "\n",
    "In order to capture holidays for public holidays , I decided to create a function entitled Kalender_Holiday showcasing all the Indian holidays, this is very important because it shades light on how events of this sort affect flight patterns . This is in two forms : \n",
    "\n",
    "- 1 : `´Passanger load aka pax load:`´ The represents the percentage of passangers on a Flight \n",
    "- 2: `´ Total Flights:`´ This represents the number of flights around a particular holidays or season ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7010c508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Day of week</th>\n",
       "      <th>Weekend (0/1)</th>\n",
       "      <th>Overlap with Weekend</th>\n",
       "      <th>Extended weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date Day of week  Weekend (0/1)  Overlap with Weekend  \\\n",
       "0 2018-01-01      Monday              0                     0   \n",
       "1 2018-01-02     Tuesday              0                     0   \n",
       "2 2018-01-03   Wednesday              0                     0   \n",
       "3 2018-01-04    Thursday              0                     0   \n",
       "4 2018-01-05      Friday              0                     0   \n",
       "5 2018-01-06    Saturday              1                     1   \n",
       "6 2018-01-07      Sunday              1                     1   \n",
       "7 2018-01-08      Monday              0                     0   \n",
       "8 2018-01-09     Tuesday              0                     0   \n",
       "9 2018-01-10   Wednesday              0                     0   \n",
       "\n",
       "   Extended weekend  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "5                 1  \n",
       "6                 1  \n",
       "7                 0  \n",
       "8                 0  \n",
       "9                 0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = \"2018-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "# Create a DataFrame with Date column\n",
    "Kalender = pd.DataFrame({\"Date\": date_range})\n",
    "\n",
    "# Define Indian holidays\n",
    "\n",
    "# Extract day of the week and create a Weekend column\n",
    "Kalender['Day of week'] = Kalender[\"Date\"].dt.day_name()\n",
    "Kalender['Weekend (0/1)'] = np.where(\n",
    "    Kalender[\"Date\"].dt.dayofweek.isin([5, 6]), 1, 0)\n",
    "Kalender['Overlap with Weekend'] = np.where(\n",
    "    Kalender[\"Date\"].dt.dayofweek.isin([5, 6]), 1, 0)\n",
    "Kalender['Extended weekend'] = np.where(\n",
    "    Kalender[\"Date\"].dt.dayofweek.isin([5, 6]), 1, 0)\n",
    "Kalender.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d823392f",
   "metadata": {
    "code_folding": [
     1,
     80,
     82
    ]
   },
   "outputs": [],
   "source": [
    "def Kalender_Holiday(Kalender_Date):\n",
    "    indian_holidays = [\n",
    "        \"2018-01-01 00:00:00\",  # New Year\n",
    "        \"2018-01-14 00:00:00\",  # Makar Sankranti / Pongal\n",
    "        \"2018-01-26 00:00:00\",  # Republic Day\n",
    "        \"2018-02-13 00:00:00\",  # Mahashivratri\n",
    "        \"2018-03-02 00:00:00\",  # Holi\n",
    "        \"2018-03-18 00:00:00\",  # Ugadi / Gudi Padwa\n",
    "        \"2018-03-30 00:00:00\",  # Good Friday\n",
    "        \"2018-04-14 00:00:00\",  # Baisakhi / Ambedkar Jayanti\n",
    "        \"2018-04-30 00:00:00\",  # Buddha Purnima\n",
    "        \"2018-06-16 00:00:00\",  # Eid al-Fitr\n",
    "        \"2018-08-15 00:00:00\",  # Independence Day\n",
    "        \"2018-08-26 00:00:00\",  # Raksha Bandhan\n",
    "        \"2018-09-03 00:00:00\",  # Janmashtami\n",
    "        \"2018-09-21 00:00:00\",  # Muharram\n",
    "        \"2018-10-02 00:00:00\",  # Gandhi Jayanti\n",
    "        \"2018-10-18 00:00:00\",  # Dussehra\n",
    "        \"2018-11-06 00:00:00\",  # Diwali\n",
    "        \"2018-11-23 00:00:00\",  # Guru Nanak Jayanti\n",
    "        \"2018-12-25 00:00:00\",  # Christmas\n",
    "        \"2019-01-01 00:00:00\",  # New Year\n",
    "        \"2019-01-15 00:00:00\",  # Makar Sankranti / Pongal\n",
    "        \"2019-01-26 00:00:00\",  # Republic Day\n",
    "        \"2019-03-04 00:00:00\",  # Mahashivratri\n",
    "        \"2019-03-21 00:00:00\",  # Holi\n",
    "        \"2019-04-06 00:00:00\",  # Ugadi / Gudi Padwa\n",
    "        \"2019-04-19 00:00:00\",  # Good Friday\n",
    "        \"2019-04-14 00:00:00\",  # Baisakhi / Ambedkar Jayanti\n",
    "        \"2019-05-18 00:00:00\",  # Buddha Purnima\n",
    "        \"2019-06-05 00:00:00\",  # Eid al-Fitr\n",
    "        \"2019-08-15 00:00:00\",  # Independence Day\n",
    "        \"2019-08-15 00:00:00\",  # Raksha Bandhan\n",
    "        \"2019-08-24 00:00:00\",  # Janmashtami\n",
    "        \"2019-09-10 00:00:00\",  # Muharram\n",
    "        \"2019-10-02 00:00:00\",  # Gandhi Jayanti\n",
    "        \"2019-10-08 00:00:00\",  # Dussehra\n",
    "        \"2019-10-27 00:00:00\",  # Diwali\n",
    "        \"2019-11-12 00:00:00\",  # Guru Nanak Jayanti\n",
    "        \"2019-12-25 00:00:00\",  # Christmas\n",
    "        \"2022-01-01 00:00:00\",  # New Year\n",
    "        \"2022-01-14 00:00:00\",  # Makar Sankranti / Pongal\n",
    "        \"2022-01-26 00:00:00\",  # Republic Day\n",
    "        \"2022-02-28 00:00:00\",  # Mahashivratri\n",
    "        \"2022-03-18 00:00:00\",  # Holi\n",
    "        \"2022-04-02 00:00:00\",  # Ugadi / Gudi Padwa\n",
    "        \"2022-04-15 00:00:00\",  # Good Friday\n",
    "        \"2022-04-14 00:00:00\",  # Baisakhi / Ambedkar Jayanti\n",
    "        \"2022-05-15 00:00:00\",  # Buddha Purnima\n",
    "        \"2022-04-02 00:00:00\",  # Eid al-Fitr\n",
    "        \"2022-08-15 00:00:00\",  # Independence Day\n",
    "        \"2022-08-11 00:00:00\",  # Raksha Bandhan\n",
    "        \"2022-08-20 00:00:00\",  # Janmashtami\n",
    "        \"2022-08-08 00:00:00\",  # Muharram\n",
    "        \"2022-10-02 00:00:00\",  # Gandhi Jayanti\n",
    "        \"2022-10-05 00:00:00\",  # Dussehra\n",
    "        \"2022-10-24 00:00:00\",  # Diwali\n",
    "        \"2022-11-08 00:00:00\",  # Guru Nanak Jayanti\n",
    "        \"2022-12-25 00:00:00\",  # Christmas\n",
    "        \"2023-01-01 00:00:00\",  # New Year\n",
    "        \"2023-01-14 00:00:00\",  # Makar Sankranti / Pongal\n",
    "        \"2023-01-26 00:00:00\",  # Republic Day\n",
    "        \"2023-02-17 00:00:00\",  # Mahashivratri\n",
    "        \"2023-03-08 00:00:00\",  # Holi\n",
    "        \"2023-03-28 00:00:00\",  # Ugadi / Gudi Padwa\n",
    "        \"2023-03-30 00:00:00\",  # Ugadi / Gudi Padwa\n",
    "        \"2023-04-07 00:00:00\",  # Good Friday\n",
    "        \"2023-04-14 00:00:00\",  # Baisakhi / Ambedkar Jayanti\n",
    "        '2023-05-05 00:00:00',  # Buddha Purnima\n",
    "        \"2023-03-28 00:00:00\",  # Eid al-Fitr\n",
    "        \"2023-08-15 00:00:00\",  # Independence Day\n",
    "        \"2023-08-30 00:00:00\",  # Raksha Bandhan\n",
    "        \"2023-08-09 00:00:00\",  # Janmashtami\n",
    "        \"2023-08-27 00:00:00\",  # Muharram\n",
    "        \"2023-10-02 00:00:00\",  # Gandhi Jayanti\n",
    "        \"2023-09-25 00:00:00\",  # Dussehra\n",
    "        \"2023-10-19 00:00:00\",  # Diwali\n",
    "        \"2023-11-26 00:00:00\",  # Guru Nanak Jayanti\n",
    "        \"2023-12-25 00:00:00\",  # Christmas\n",
    "    ]\n",
    "    if str(Kalender_Date) in indian_holidays:\n",
    "        return 1\n",
    "    else:\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58567162",
   "metadata": {
    "code_folding": [
     7,
     13
    ]
   },
   "outputs": [],
   "source": [
    "# Apply the Kalender_Holiday function to the 'Date' column to get the holiday indicator\n",
    "Kalender['Holiday (0/1)'] = Kalender['Date'].apply(Kalender_Holiday)\n",
    "\n",
    "# Identify the rows where the 'Holiday' column is 1\n",
    "holiday_rows = Kalender[Kalender['Holiday (0/1)'] == 1].index\n",
    "\n",
    "# Update the 'Extended weekend' column based on the condition you specified\n",
    "for index in holiday_rows:\n",
    "    if index - 1 in Kalender.index:\n",
    "        Kalender.at[index - 1, 'Extended weekend'] = 1\n",
    "    if index + 1 in Kalender.index:\n",
    "        Kalender.at[index + 1, 'Extended weekend'] = 1\n",
    "\n",
    "# Drop the 'Holiday' column if you don't need it anymore\n",
    "#Kalender = Kalender.drop(columns=['Holiday'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "209da570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column entitled Festival representing Public holidays.\n",
    "Kalender['Festival'] = Kalender['Date'].apply(Kalender_Holiday)\n",
    "# Converting the Date column to a pandas datetime format\n",
    "Kalender['Date'] = Kalender['Date'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd331db",
   "metadata": {},
   "source": [
    "### 1.5 Adding 6 more columns to a the Data Frame for pattern study \n",
    "\n",
    "In order to study how holidays influence passanger flight patterns , I added 6 new rows representing . In this way, we will be able to group flight numbers and load factors w.r.t the created days .\n",
    "\n",
    "- 1:`´1day_after :`´ This represents the **1st day** after a public holiday .\n",
    "- 2:`´2day_after :`´ This represents the **2nd day** after a public holiday .\n",
    "- 3:`´3day_after :`´ This represents the **3rd day** after a public holiday .\n",
    "\n",
    "- 4:`´1day_before :`´ This represents the **1st day** before a public holiday .\n",
    "- 5:`´2day_before :`´ This represents the **2nd day** before a public holiday .\n",
    "- 6:`´3day_before :`´ This represents the **3rd day** before a public holiday .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "664d648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column '1day_after' and initialize it with 0\n",
    "Kalender['1day_after'] = 0\n",
    "\n",
    "# Identify rows where 'Holiday' column is 1\n",
    "holiday_rows = Kalender['Holiday (0/1)'] == 1\n",
    "\n",
    "# Set '1day_after' to 1 for the day after a holiday\n",
    "Kalender.loc[holiday_rows.shift(1, fill_value=False), '1day_after'] = 1\n",
    "Kalender['1day_after'] = Kalender['1day_after'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5f027a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column '2day_after' and initialize it with 0\n",
    "Kalender['2day_after'] = 0\n",
    "\n",
    "# Identify rows where 'Holiday' column is 1\n",
    "holiday_rows = Kalender['Holiday (0/1)'] == 1\n",
    "\n",
    "# Set '2day_after' to 2 for the day after a holiday\n",
    "Kalender.loc[holiday_rows.shift(2, fill_value=False), '2day_after'] = 1\n",
    "Kalender['2day_after'] = Kalender['2day_after'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7084b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column '3day_after' and initialize it with 0\n",
    "Kalender['3day_after'] = 0\n",
    "\n",
    "# Identify rows where 'Holiday' column is 1\n",
    "holiday_rows = Kalender['Holiday (0/1)'] == 1\n",
    "\n",
    "# Set '3day_after' to 1 for the day before a holiday\n",
    "Kalender.loc[holiday_rows.shift(3, fill_value=False), '3day_after'] = 1\n",
    "Kalender['3day_after'] = Kalender['3day_after'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97b682da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kalender['4day_after'] = 0\n",
    "\n",
    "# Identify rows where 'Holiday' column is 1\n",
    "holiday_rows = Kalender['Holiday (0/1)'] == 1\n",
    "\n",
    "# Set '4day_after' to 1 for the day after a holiday\n",
    "Kalender.loc[holiday_rows.shift(4, fill_value=False), '4day_after'] = 1\n",
    "Kalender['4day_after'] = Kalender['4day_after'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "273d2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kalender['1day_before'] = 0\n",
    "\n",
    "# Identify rows where 'Holiday' column is 1\n",
    "holiday_rows = Kalender['Holiday (0/1)'] == 1\n",
    "\n",
    "# Set '1day_before' to 1 for the day before a holiday\n",
    "Kalender.loc[holiday_rows.shift(-1, fill_value=False), '1day_before'] = 1\n",
    "Kalender['1day_before'] = Kalender['1day_before'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "644c19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kalender['2day_before'] = 0\n",
    "\n",
    "# Identify rows where 'Holiday' column is 1\n",
    "holiday_rows = Kalender['Holiday (0/1)'] == 1\n",
    "\n",
    "# Set '2day_before' to 1 for the day before a holiday\n",
    "Kalender.loc[holiday_rows.shift(-2, fill_value=False), '2day_before'] = 1\n",
    "Kalender['2day_before'] = Kalender['2day_before'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96671734",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kalender['3day_before'] = 0\n",
    "\n",
    "# Identify rows where 'Holiday' column is 1\n",
    "holiday_rows = Kalender['Holiday (0/1)'] == 1\n",
    "\n",
    "# Set '3day_before' to 1 for the day before a holiday\n",
    "Kalender.loc[holiday_rows.shift(-3, fill_value=False), '3day_before'] = 1\n",
    "Kalender['3day_before'] = Kalender['3day_before'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a61526f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kalender['4day_before'] = 0\n",
    "\n",
    "# Identify rows where 'Holiday' column is 1\n",
    "holiday_rows = Kalender['Holiday (0/1)'] == 1\n",
    "\n",
    "# Set '4day_before' to 1 for the day before a holiday\n",
    "Kalender.loc[holiday_rows.shift(-4, fill_value=False), '4day_before'] = 1\n",
    "Kalender['4day_before'] = Kalender['4day_before'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57b9c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging both the Arrival . Depature data frames with the  Kalender Data Frame.\n",
    "NF_Arrival_Calender = pd.merge(NF_Arrival, Kalender, on=\"Date\", how=\"inner\")\n",
    "NF_Depature_Calender=pd.merge(NF_Depature,Kalender,on=\"Date\",how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9710902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_int_fillna(df, column_name):\n",
    "    ''' Converts the specified column to integers and fills null values with 0 '''\n",
    "    try:\n",
    "        df[column_name] = df[column_name].apply(pd.to_numeric, errors='coerce')\n",
    "        df[column_name] = df[column_name].fillna(0).astype(int)\n",
    "    except ValueError:\n",
    "        # Handle cases where conversion fails (e.g., non-numeric strings)\n",
    "        # You can customize this part based on your requirements\n",
    "        pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fbe40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NF_Arrival_Calender = convert_column_int_fillna(NF_Arrival_Calender,\n",
    "                                                 'all_capacity_pax')\n",
    "NF_Arrival_Calender = convert_column_int_fillna(NF_Arrival_Calender,\n",
    "                                                 'all_boarded_pax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99affa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NF_Depature_Calender = convert_column_int_fillna(NF_Depature_Calender,\n",
    "                                                 'all_capacity_pax')\n",
    "NF_Depature_Calender = convert_column_int_fillna(NF_Depature_Calender,\n",
    "                                                 'all_boarded_pax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3897b46",
   "metadata": {},
   "source": [
    " ## 2.0 Second section of Data cleaning and preparation\n",
    "- I did recieve another dirty dirty fame that  also needed a fresh cleaning before the two data sets could be put together \n",
    "\n",
    "- This includes renaming and deleting of unecessary columns , removal of duplicates,column merge ups , Date conversions among others . \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174ece7",
   "metadata": {},
   "source": [
    "### 2.1 Importing the First DB into jupyter Notebook"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d528d46",
   "metadata": {},
   "source": [
    "UFIS2018 = pd.read_excel(\"UFIS2018.xlsx\") \n",
    "UFIS2019 = pd.read_excel(\"UFIS2019.xlsx\")\n",
    "UFIS2022 = pd.read_excel(\"UFIS2022.xlsx\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a767dd3",
   "metadata": {},
   "source": [
    "# storing the DataFrames in a list\n",
    "list_df = [UFIS2018, UFIS2019, UFIS2022]\n",
    "\n",
    "# Then initialize an empty Data Frame\n",
    "UFISAll = pd.DataFrame()\n",
    "\n",
    "for files in list_df:\n",
    "    UFISAll = pd.concat([files,UFISAll],\n",
    "                        ignore_index=True)  # file being saved into a list"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce2cd172",
   "metadata": {},
   "source": [
    "# Saving it to a parquet format so that next time I have to open the files, a lot of time will be saved.\n",
    "UFISAll = UFISAll.to_parquet(\n",
    "    r'D:\\DEHLI Machine Learning Projects\\AOBD_Historical_Data\\UFISAll.parquet'\n",
    ")  # Saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11d69887",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll = pd.read_parquet(\n",
    "    r'D:\\DEHLI Machine Learning Projects\\AOBD_Historical_Data\\UFISAll.parquet'\n",
    ")  # Saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651c3dd",
   "metadata": {},
   "source": [
    "### 2.2 This section contains the following functions:\n",
    "- Selection of important colimns \n",
    "- Conversion of Dates \n",
    "- Merging of flight schedules with the airline Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1442cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll.columns = [\n",
    "    \"ADID\", 'FLNO', \"ALC3\", \"HOPO\", \"STOA\", \"STOD\", \"ONBL\", \"OFBL\", \"REGN\",\n",
    "    \"ACT3\", \"STYP\", \"FTYP\", \"FLTI\", \"STEV\", \"PAXT\", \"RWYA\", \"RWYD\", \"URNO\",\n",
    "    \"RKEY\", \"ALC2\", \"ORG3\", \"DES3\", \"AIRB\", \"LAND\", \"PSTD\", \"PSTA\", \"GTA1\",\n",
    "    \"GTD1\", \"BLT1\", \"CKIT\", \"CKIF\", \"PAXF\", \"FLDA\"\n",
    "]  # namign the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f85841dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_two_columns(df, new_column, column_1, column_2):\n",
    "    df[new_column] = df[column_1] + df[column_2]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a721138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_trailings(df, column_name):\n",
    "    df[column_name] = df[column_name].str.replace('\\n', '')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "069544ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_unimportant_features(df, feature_one, feature_two, feature_three,\n",
    "                                feature_four, feature_five, feature_six,\n",
    "                                feature_seven, feature_eight, feature_nine,\n",
    "                                feature_ten, feature_eleven, feature_twelve,\n",
    "                                feature_thirteen, feature_fourteen,\n",
    "                                feature_fifteen, feature_sixteen):\n",
    "    '''\n",
    "    This function takes in a DataFrame and removes specified columns that are not important for deciding\n",
    "    a client's ability to receive credit. columns such as Names, customer_ID etc., do not play a part. \n",
    "    Users can specify additional features to drop out , more features to be removed will be taken care as time goes\n",
    "    and this is very important for both of us . \n",
    "    '''\n",
    "    df = df.drop(columns=[\n",
    "        feature_one, feature_two, feature_three, feature_four, feature_five,\n",
    "        feature_six, feature_seven, feature_eight, feature_nine, feature_ten,\n",
    "        feature_eleven, feature_twelve, feature_thirteen, feature_fourteen,\n",
    "        feature_fifteen, feature_sixteen\n",
    "    ],\n",
    "                 inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbd91f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_two_columns(UFISAll, \"ALACT\", \"ALC2\", \"ACT3\")\n",
    "remove_unwanted_trailings(UFISAll, \"STOA\")\n",
    "delete_unimportant_features(UFISAll, \"RWYA\",\"RWYD\",\"URNO\",\"RKEY\",\"AIRB\",\"LAND\",\"PSTD\",\"PSTA\",\"GTA1\",\"GTD1\",\"BLT1\",\"CKIT\",\"CKIF\",\"PAXF\",\"HOPO\",\"ALC3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc88bb",
   "metadata": {},
   "source": [
    "### 2.3 This section includes :\n",
    "- converting the date time columns into a pandas date time format\n",
    "- Creation of the local Inidian time and date \n",
    "- Removing irrelevant Flights form the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3a9ae",
   "metadata": {},
   "source": [
    "####  Filter out empty strings and convert to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e91986b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_convert_datetime(df, column_name):\n",
    "    '''\n",
    "    The goal of this function is to remove empty strings and spaces from the charachters, \n",
    "    it also converts these specific columns into a datetime pandas object \n",
    "    '''\n",
    "    df[column_name] = df[column_name].astype(str).str.replace('\\n',\n",
    "                                                              '').str.strip()\n",
    "    df[column_name] = df[column_name][df[column_name] != ''].apply(\n",
    "        lambda x: pd.to_datetime(x, format='%Y%m%d%H%M%S', errors='coerce'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d46587c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_int_date(df, column_name):\n",
    "    df[column_name] = pd.to_datetime(df[column_name], format='%Y%m%d')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41ad43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll = remove_empty_convert_datetime(UFISAll, \"STOA\")\n",
    "UFISAll = remove_empty_convert_datetime(UFISAll, \"STOD\")\n",
    "UFISAll = remove_empty_convert_datetime(UFISAll, \"ONBL\")\n",
    "UFISAll = remove_empty_convert_datetime(UFISAll, \"OFBL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60689410",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll = convert_int_date(UFISAll,\"FLDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8b22604",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll=UFISAll.rename(columns={\"FLDA\":\"Date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47dad4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll[\"STOA_UCT\"] = UFISAll[\"STOA\"].apply(lambda x: timeChange(x)) \n",
    "UFISAll[\"STOD_UCT\"] = UFISAll[\"STOD\"].apply(lambda x: timeChange(x)) \n",
    "UFISAll[\"ONBL_UCT\"] = UFISAll[\"ONBL\"].apply(lambda x: timeChange(x))  \n",
    "UFISAll[\"OFBL_UCT\"] = UFISAll[\"OFBL\"].apply(lambda x: timeChange(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40e457e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll=UFISAll.drop(columns=['STOA','STOD','ONBL','OFBL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1fccff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_releveant_legtype(df, column_name, Arrival, Depature):\n",
    "    '''\n",
    "    The goal of this function is to select only relevant leg types which are Depature and Arrival flights . \n",
    "    '''\n",
    "    df = df[(df[column_name] == Arrival) | (df[column_name] == Depature)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c35ea5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll = select_releveant_legtype(UFISAll,'ADID','A','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43858c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll = correct_terminals_name(UFISAll,\"STEV\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be4eb8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll = delete_incorrect_terminals(UFISAll,\"STEV\") #it should return a df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7799faf",
   "metadata": {},
   "source": [
    "### 2.4 : This section includes :\n",
    "- Removing irrelevant Service types \n",
    "- Removing cancelled flights from the data frame \n",
    "- Removing duplicate values \n",
    "- Merging the Flight and air craft type Data Frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b3609dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_service_types(df, column_name):\n",
    "    result_df = df[(df[column_name] == 'C ') | (df[column_name] == 'G ') |\n",
    "                   (df[column_name] == 'J ') | (df[column_name] == 'L ')]\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95fa16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll = important_service_types(UFISAll,'STYP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c9f0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_cancelled_flights(df, column_name):\n",
    "    '''\n",
    "    The goal of this function to delete out all flights under the type O from the Flight Schedule Data Frame,\n",
    "    Flights with O type are represented as cancelled flights .\n",
    "    '''\n",
    "    result_df = df[(df[column_name] != 'O')]\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b9107dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_spaces(df, column_name):\n",
    "    ''' The goal of this function is to remove leading and trailing spaces,\n",
    "    remove rows where the column contains only spaces or is empty,\n",
    "    and remove rows with NaN values in the column. '''\n",
    "\n",
    "    # Remove leading and trailing spaces from the specified column\n",
    "    df[column_name] = df[column_name].str.strip()\n",
    "\n",
    "    # Filter out rows where the column is either empty or contains only spaces\n",
    "    df = df[(df[column_name] != '') | (df[column_name] != ' ')]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f27bc96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll = delete_cancelled_flights(UFISAll, 'FTYP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3978549",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll = delete_spaces(UFISAll,'ALC2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ede5caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "UFISAll= UFISAll.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b881d3",
   "metadata": {},
   "source": [
    "####  <span style='color:blue'>1.4 (Reading Air craft + Airline Records from the DB1) </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f674a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Calender file for mapping purposes\n",
    "Aircraft_Sit = pd.read_csv(\n",
    "    r\"D:\\DEHLI Machine Learning Projects\\AOBD_Historical_Data\\AirlineSitCapacity.csv\",\n",
    "    delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55c0c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aircraft_Sit['ALACT'] = Aircraft_Sit['Airline'] + Aircraft_Sit['AcftType']\n",
    "Aircraft_INFO = Aircraft_Sit.rename(\n",
    "    columns={\"AcftType\": \"ACT3\"})  # is to be paired on the second section ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3c2f7",
   "metadata": {},
   "source": [
    "####  <span style='color:blue'>1.41 (Reading Air craft + Airline Records from the DB2) </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa67dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Another Airlines - AirlineSitCapacity data set\n",
    "Aircraft_Default = pd.read_excel(\n",
    "    r\"D:\\DEHLI Machine Learning Projects\\AOBD_Historical_Data\\AirlineSitCapacity_Default.xlsx\"\n",
    ")\n",
    "Aircraft_Default = Aircraft_Default.rename(columns={\"ACT3\": \"ACT3_x\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "194620c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to handle mixed data types\n",
    "def convert_to_string(value):\n",
    "    '''\n",
    "    The goal of the function is to convert the Aircraft type to an interger.\n",
    "    '''\n",
    "    if isinstance(value, (int, float)):\n",
    "        return str(value)\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d260bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aircraft_Default[\"ACT3_x\"]=Aircraft_Default[\"ACT3_x\"].apply(convert_to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d2e87",
   "metadata": {},
   "source": [
    "# 3.0 <span style='color:blue'> Inner Joing the two Files together</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fcb723c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FSDS=pd.merge(UFISAll,Kalender,on=\"Date\",how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1048f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Dataframes FSDS and Aircraft_INFO\n",
    "FDB_1=pd.merge(FSDS,Aircraft_INFO,on='ALACT',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1b58746",
   "metadata": {},
   "outputs": [],
   "source": [
    "FDB_B=FDB_1[FDB_1[\"ACT3_y\"].isnull() & FDB_1[\"Seats\"].isnull() & FDB_1[\"Airline\"].isnull()]\n",
    "Second_DB=FDB_B.drop(columns=[\"Seats\",\"ACT3_y\",\"Airline\"])\n",
    "Second_DB[\"ACT3_x\"]=Second_DB[\"ACT3_x\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b459061",
   "metadata": {},
   "outputs": [],
   "source": [
    "FDB=pd.merge(Second_DB,Aircraft_Default,on=\"ACT3_x\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e0e07bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Dataframes FSDS and Aircraft_INFO\n",
    "FDB_New=pd.merge(FSDS,Aircraft_INFO,on='ALACT',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "161a4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_DB1=FDB_New[['Date','Day of week', 'Weekend (0/1)', 'Holiday (0/1)',\n",
    "       'Festival', 'Overlap with Weekend', 'Extended weekend','1day_after',\n",
    "       '2day_after', '3day_after', '1day_before', '2day_before', '3day_before',\n",
    "       '4day_before', '4day_after','ADID','STYP','PAXT','ORG3','DES3','STOA_UCT','STOD_UCT','ONBL_UCT','OFBL_UCT','Seats','FLTI',\"FLNO\",'STEV',\"ALC2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd9febd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_DB2=FDB[['Date','Day of week', 'Weekend (0/1)', 'Holiday (0/1)',\n",
    "       'Festival', 'Overlap with Weekend', 'Extended weekend','1day_after',\n",
    "       '2day_after', '3day_after', '1day_before', '2day_before', '3day_before',\n",
    "       '4day_before', '4day_after','ADID','STYP','PAXT','ORG3','DES3','STOA_UCT','STOD_UCT','ONBL_UCT','OFBL_UCT','Seats','FLTI','FLNO','STEV',\"ALC2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb7266",
   "metadata": {},
   "source": [
    "## 3.1 Concatenating the two DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a01c73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_DBINFO=pd.concat([Flight_DB1,Flight_DB2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7a07d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_DBINFO_Arrival = Flight_DBINFO[Flight_DBINFO[\"ADID\"] == 'A'][[\"Date\", \"Day of week\", \"Weekend (0/1)\", \"Holiday (0/1)\", 'Festival', 'Overlap with Weekend', 'Extended weekend','1day_after',\n",
    "       '2day_after', '3day_after', '1day_before', '2day_before', '3day_before',\n",
    "       '4day_before', '4day_after', \"PAXT\", \"ORG3\", \"DES3\", \"STOA_UCT\", \"Seats\", \"FLTI\",'FLNO','STEV',\"ALC2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6aabcdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_DBINFO_Departure = Flight_DBINFO[Flight_DBINFO[\"ADID\"] == 'D'][[\"Date\", \"Day of week\", \"Weekend (0/1)\", \"Holiday (0/1)\", 'Festival', 'Overlap with Weekend', 'Extended weekend', '1day_after',\n",
    "       '2day_after', '3day_after', '1day_before', '2day_before', '3day_before',\n",
    "       '4day_before', '4day_after',\"PAXT\", \"ORG3\", \"DES3\", \"STOD_UCT\", \"Seats\", \"FLTI\",'FLNO','STEV',\"ALC2\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2eb2fd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of Arrival is : 24279\n",
      "shape of Depature is : 21945\n"
     ]
    }
   ],
   "source": [
    "print('shape of Arrival is :',len(Flight_DBINFO_Arrival))\n",
    "print('shape of Depature is :',len(Flight_DBINFO_Departure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd8b43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NF_Depature_Calender=NF_Depature_Calender[['Date', 'Day of week', 'Weekend (0/1)', 'Holiday (0/1)', 'Festival',\n",
    "       'Overlap with Weekend', 'Extended weekend','1day_after',\n",
    "       '2day_after', '3day_after', '1day_before', '2day_before', '3day_before',\n",
    "       '4day_before', '4day_after',\"all_boarded_pax\",\"origin_airport_iata\",\"destination_airport_iata\",\"sobt_uct\",\"all_capacity_pax\",\"traffic_type\",'flight_number','public_terminal',\"airline_iata\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95de861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NF_Arrival_Calender=NF_Arrival_Calender[['Date', 'Day of week', 'Weekend (0/1)', 'Holiday (0/1)', 'Festival',\n",
    "       'Overlap with Weekend', 'Extended weekend','1day_after',\n",
    "       '2day_after', '3day_after', '1day_before', '2day_before', '3day_before',\n",
    "       '4day_before', '4day_after',\"all_boarded_pax\",\"origin_airport_iata\",\"destination_airport_iata\",\"sibt_uct\",\"all_capacity_pax\",\"traffic_type\",'flight_number','public_terminal',\"airline_iata\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da70c0eb",
   "metadata": {},
   "source": [
    "### 3.2 Renaming the columns for all the 4 DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3b37154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_DBINFO_Arrival=Flight_DBINFO_Arrival.rename(columns={\"PAXT\":\"boarded pax\",'ORG3':\"Original Airport\",'DES3':\"Destination airport\",\"STOA_UCT\":\"Arrival Datetime\",'Seats':\"Seat Capacity\",'FLTI':\"traffic_type\",\"FLNO\":\"Flight Number\",\"STEV\":\"Terminal\",\"ALC2\":\"Airline Code\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "05493884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_strips(df, column_name):\n",
    "    df[column_name] = df[column_name].str.replace(\" \", \"\").str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ca56de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_DBINFO_Arrival = remove_strips(Flight_DBINFO_Arrival, \"Flight Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5e656",
   "metadata": {},
   "source": [
    "- Renaming the NF_Arrival_Calender columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "68714187",
   "metadata": {},
   "outputs": [],
   "source": [
    "NF_Arrival_Calender = NF_Arrival_Calender.rename(\n",
    "    columns={\n",
    "        \"all_boarded_pax\": \"boarded pax\",\n",
    "        'origin_airport_iata': \"Original Airport\",\n",
    "        'destination_airport_iata': \"Destination airport\",\n",
    "        \"sibt_uct\": \"Arrival Datetime\",\n",
    "        'all_capacity_pax': \"Seat Capacity\",\n",
    "        'leg_type': \"traffic_type\",\n",
    "        'flight_number': \"Flight Number\",\n",
    "        'public_terminal': \"Terminal\",\n",
    "        \"airline_iata\": \"Airline Code\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a94a6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_DBINFO_Arrival = convert_column_int_fillna(Flight_DBINFO_Arrival,\n",
    "                                                 'Seat Capacity')\n",
    "Flight_DBINFO_Arrival = convert_column_int_fillna(Flight_DBINFO_Arrival,\n",
    "                                                 'boarded pax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8498e",
   "metadata": {},
   "source": [
    "#### 3.2.1 Concatenating the two Arrival DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4bf8de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Arrival_DF = pd.concat([Flight_DBINFO_Arrival, NF_Arrival_Calender],\n",
    "                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fa9f6117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Day of week', 'Weekend (0/1)', 'Holiday (0/1)', 'Festival',\n",
       "       'Overlap with Weekend', 'Extended weekend', '1day_after', '2day_after',\n",
       "       '3day_after', '1day_before', '2day_before', '3day_before',\n",
       "       '4day_before', '4day_after', 'boarded pax', 'Original Airport',\n",
       "       'Destination airport', 'Arrival Datetime', 'Seat Capacity',\n",
       "       'traffic_type', 'Flight Number', 'Terminal', 'Airline Code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flight_DBINFO_Arrival.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "75eb4dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Day of week', 'Weekend (0/1)', 'Holiday (0/1)', 'Festival',\n",
       "       'Overlap with Weekend', 'Extended weekend', '1day_after', '2day_after',\n",
       "       '3day_after', '1day_before', '2day_before', '3day_before',\n",
       "       '4day_before', '4day_after', 'boarded pax', 'Original Airport',\n",
       "       'Destination airport', 'Arrival Datetime', 'Seat Capacity',\n",
       "       'traffic_type', 'Flight Number', 'Terminal', 'Airline Code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NF_Arrival_Calender.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e3b596c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363575, 24)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Arrival_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6bd83ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_DBINFO_Departure = Flight_DBINFO_Departure.rename(\n",
    "    columns={\n",
    "        \"PAXT\": \"boarded pax\",\n",
    "        'ORG3': \"Original Airport\",\n",
    "        'DES3': \"Destination airport\",\n",
    "        \"STOD_UCT\": \"Depature Datetime\",\n",
    "        'Seats': \"Seat Capacity\",\n",
    "        'FLTI': \"traffic_type\",\n",
    "        \"FLNO\": \"Flight Number\",\n",
    "        \"STEV\": \"Terminal\",\n",
    "        \"ALC2\": \"Airline Code\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b8ba65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_DBINFO_Departure = remove_strips(Flight_DBINFO_Departure,\n",
    "                                        \"Flight Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "864338d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NF_Depature_Calender = NF_Depature_Calender.rename(\n",
    "    columns={\n",
    "        \"all_boarded_pax\": \"boarded pax\",\n",
    "        'origin_airport_iata': \"Original Airport\",\n",
    "        'destination_airport_iata': \"Destination airport\",\n",
    "        \"sobt_uct\": \"Depature Datetime\",\n",
    "        'all_capacity_pax': \"Seat Capacity\",\n",
    "        'leg_type': \"traffic_type\",\n",
    "        'flight_number': \"Flight Number\",\n",
    "        \"public_terminal\": \"Terminal\",\n",
    "        \"airline_iata\": \"Airline Code\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "16e23482",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_DBINFO_Departure = convert_column_int_fillna(Flight_DBINFO_Departure,\n",
    "                                                 'Seat Capacity')\n",
    "Flight_DBINFO_Departure = convert_column_int_fillna(Flight_DBINFO_Departure,\n",
    "                                                 'boarded pax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc98bca",
   "metadata": {},
   "source": [
    "#### 3.2.1 Concatenating the two Depature DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "72df0f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Depature_DF = pd.concat([Flight_DBINFO_Departure, NF_Depature_Calender],\n",
    "                        ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bcf66ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'Arrival_DF' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store Arrival_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e0aa019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'Depature_DF' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store Depature_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59a9b076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the end time\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6448dc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time is : 1577.27 seconds\n",
      "Total excecition time is : 26.29 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = end_time - start_time\n",
    "print(f\"Total execution time is : {total_time:.2f} seconds\")\n",
    "print(f\"Total excecition time is : {total_time/60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
